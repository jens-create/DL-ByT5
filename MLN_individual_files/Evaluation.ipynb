{"cells":[{"cell_type":"markdown","metadata":{"id":"_kbkVzu9QNnN"},"source":["# INSTALLS AND IMPORTS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3216,"status":"ok","timestamp":1638801784214,"user":{"displayName":"Emil helgren","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16470713661505960408"},"user_tz":-60},"id":"cMbv7LB0n7je","outputId":"52f12127-e458-401d-bc72-4089eb83d81a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (0.12.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n"]}],"source":["### INSTALL DEPENDENCIES\n","!pip install python-Levenshtein\n","!pip uninstall nltk\n","!pip install nltk==3.6.2"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"yeUjTW4lm06J"},"outputs":[],"source":["### IMPORT LIBRARIES\n","import pandas as pd\n","import numpy as np\n","import sys\n","import os\n","import Levenshtein as Lev\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"P7Pk_j9bQL8N"},"source":["# FUNKTIONER"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"hA-n0YQOrd0B"},"outputs":[],"source":["def wer(s1, s2):\n","    \"\"\"\n","    Computes the Word Error Rate, defined as the edit distance between the\n","    two provided sentences after tokenizing to words.\n","    Arguments:\n","        s1 (string): space-separated sentence\n","        s2 (string): space-separated sentence\n","    \"\"\"\n","\n","    # build mapping of words to integers\n","    b = set(s1.split() + s2.split())\n","    word2char = dict(zip(b, range(len(b))))\n","\n","    # map the words to a char array (Levenshtein packages only accepts\n","    # strings)\n","    w1 = [chr(word2char[w]) for w in s1.split()]\n","    w2 = [chr(word2char[w]) for w in s2.split()]\n","\n","    return Lev.distance(''.join(w1), ''.join(w2))\n","\n","def wer_normalized(s1, s2):\n","  return wer(s1.lower(),s2.lower()) / len(s2.split(\" \"))\n","def calculate_wer(df):\n","  return wer_normalized(df['corrected'],df['reference_text'])"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Y1_ZR4Tql6jf"},"outputs":[],"source":["def open_dataset(path, load_outputs=True):\n","    with open(path) as f:\n","        sentences = f.read().split(\"\\n\\n\")[:-1]\n","    sentences = [s.split('\\n') for s in sentences]\n","    inputs = [[w.split('\\t')[0] for w in s] for s in sentences]\n","\n","    if not load_outputs:\n","        return inputs\n","\n","    outputs = [[w.split('\\t')[1] for w in s] for s in sentences]\n","    return inputs, outputs"]},{"cell_type":"markdown","metadata":{"id":"hCqaGmZWQSGY"},"source":["# LOAD DATA and EVALUATE ON MultiLexNorm\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"_4txVDGrp7uc"},"outputs":[],"source":["### READ FIL\n","\n","_, outputs = open_dataset('/Users/jenspt/Desktop/git/DL-ByT5/data/outputs_mln_ft.txt')\n","corrected = [' '.join(sentence) for sentence in outputs]\n","transcribed = [' '.join(sentence) for sentence in _]"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"qh60FHdGxVYb"},"outputs":[],"source":["with open('/Users/jenspt/Desktop/git/DL-ByT5/data/mln_data_test_outputs.pkl', 'rb') as f:\n","    reference = pickle.load(f)\n","\n","reference = [\" \".join(s) for s in reference]"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"7haEPlfGNF08"},"outputs":[],"source":["mln_df = pd.DataFrame(list(zip(reference, transcribed, corrected)),\n","               columns =['reference_text', 'transcription', 'corrected'])"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1638802548459,"user":{"displayName":"Emil helgren","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16470713661505960408"},"user_tz":-60},"id":"XrwfiYxaNijl","outputId":"ae84733f-bda9-4af5-e8f9-772659e1190b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reference_text</th>\n","      <th>transcription</th>\n","      <th>corrected</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>gyset der har siddet sammenkrøbet i nakkeregio...</td>\n","      <td>gyset der har siddet sammenkrøbet i nakke regi...</td>\n","      <td>gylfi dér er sidde sammenkrøbene inden nakkere...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>det er et enormt befolkningstal sammenlignet m...</td>\n","      <td>det er et enormt befolkningstal sammenlignet m...</td>\n","      <td>der er enormt enorm befolkningstal sammenligne...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>de seks balletter er ikke alle avantgardestykk...</td>\n","      <td>de seks balletter er ikke alle avangard stykke...</td>\n","      <td>det seksballetter balletter har alle al avanga...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>stakkels davedarling</td>\n","      <td>stakkels dave darling</td>\n","      <td>stakels davedarling darling</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>det får han osse</td>\n","      <td>det får han også</td>\n","      <td>dét for hr osse</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                      reference_text  \\\n","0  gyset der har siddet sammenkrøbet i nakkeregio...   \n","1  det er et enormt befolkningstal sammenlignet m...   \n","2  de seks balletter er ikke alle avantgardestykk...   \n","3                              stakkels davedarling    \n","4                                   det får han osse   \n","\n","                                       transcription  \\\n","0  gyset der har siddet sammenkrøbet i nakke regi...   \n","1  det er et enormt befolkningstal sammenlignet m...   \n","2  de seks balletter er ikke alle avangard stykke...   \n","3                              stakkels dave darling   \n","4                                   det får han også   \n","\n","                                           corrected  \n","0  gylfi dér er sidde sammenkrøbene inden nakkere...  \n","1  der er enormt enorm befolkningstal sammenligne...  \n","2  det seksballetter balletter har alle al avanga...  \n","3                        stakels davedarling darling  \n","4                                    dét for hr osse  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["mln_df.head()"]},{"cell_type":"markdown","metadata":{"id":"gxKNUAU6grWB"},"source":["# Evaluation\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"QXcp0E_bh11o"},"outputs":[],"source":["mln_df['corrected_wer'] = mln_df.apply(calculate_wer,axis=1)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MLN WER:  75.16467803146487\n"]}],"source":["print(\"MLN WER: \", mln_df['corrected_wer'].mean()*100)"]},{"cell_type":"markdown","metadata":{"id":"xr47s4vB_PfT"},"source":["# BLEU & GLEU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7KJbNDPj_RxO"},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate.gleu_score import sentence_gleu"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":287,"status":"ok","timestamp":1638803594063,"user":{"displayName":"Emil helgren","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16470713661505960408"},"user_tz":-60},"id":"NZuzXfjH_VPy"},"outputs":[],"source":["def calculate_bleu_baseline_normalized(df):\n","  total_bleu = 0\n","  for i in range(len(df)):\n","    ref = [df['reference_text'].iloc[i].split(\" \")]\n","    hyp = df['transcription'].iloc[i].split(\" \")\n","    sentence_bleu_score = sentence_bleu(ref, hyp)\n","    total_bleu += sentence_bleu_score\n","  return total_bleu / len(df)\n","\n","def calculate_gleu_baseline_normalized(df):\n","  total_gleu = 0\n","  for i in range(len(df)):\n","    ref = [df['reference_text'].iloc[i].split(\" \")]\n","    hyp = df['transcription'].iloc[i].split(\" \")\n","    sentence_gleu_score = sentence_gleu(ref, hyp)\n","    total_gleu += sentence_gleu_score\n","  return total_gleu / len(df)\n","\n","def calculate_bleu_normalized(df):\n","  total_bleu = 0\n","  for i in range(len(df)):\n","    ref = [df['reference_text'].iloc[i].split(\" \")]\n","    hyp = df['corrected'].iloc[i].split(\" \")\n","    sentence_bleu_score = sentence_bleu(ref, hyp)\n","    total_bleu += sentence_bleu_score\n","  return total_bleu / len(df)\n","\n","def calculate_gleu_normalized(df):\n","  total_gleu = 0\n","  for i in range(len(df)):\n","    ref = [df['reference_text'].iloc[i].split(\" \")]\n","    hyp = df['corrected'].iloc[i].split(\" \")\n","    sentence_gleu_score = sentence_gleu(ref, hyp)\n","    total_gleu += sentence_gleu_score\n","  return total_gleu / len(df)"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3725,"status":"ok","timestamp":1638803599785,"user":{"displayName":"Emil helgren","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16470713661505960408"},"user_tz":-60},"id":"NWRzSOwJT_Vp","outputId":"85ca75b3-d5bf-447f-d08e-37014a011e07"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"name":"stdout","output_type":"stream","text":["0.6473379750494206\n","0.7097791013001765\n"]}],"source":["print(calculate_bleu_baseline_normalized(mln_df))\n","print(calculate_gleu_baseline_normalized(mln_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3533,"status":"ok","timestamp":1638802577563,"user":{"displayName":"Emil helgren","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16470713661505960408"},"user_tz":-60},"id":"etxyYE_o_n7s","outputId":"987bd61c-568d-49a5-b07c-e4dc7c2d48ed"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Normalized bleu:\n","0.006133337391333845\n","\n","Normalized gleu:\n","0.1129387766622926\n"]}],"source":["\n","#corpus_bleu_score = calculate_bleu_corpus(df)\n","norm_bleu_score = calculate_bleu_normalized(mln_df)\n","#calculated_gleu = calculate_gleu(df) #RuntimeError: generator raised StopIteration\n","\n","\n","#print(\"\\nCorpus bleu:\")\n","#print(corpus_bleu_score)\n","print(\"\\nNormalized bleu:\")\n","print(norm_bleu_score)\n","#print(calculated_gleu)\n","\n","#corpus_bleu_score = calculate_bleu_corpus(df)\n","norm_gleu_score = calculate_gleu_normalized(mln_df)\n","#calculated_gleu = calculate_gleu(df) #RuntimeError: generator raised StopIteration\n","\n","\n","#print(\"\\nCorpus bleu:\")\n","#print(corpus_bleu_score)\n","print(\"\\nNormalized gleu:\")\n","print(norm_gleu_score)\n","#print(calculated_gleu)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGMZ8CdjDthB"},"outputs":[],"source":["## GLEU SCORES\n","# Baseline (transcription)= 0.8534873003494211\n","# ByT5_lr_1e4             = 0.8952613840069592\n","# ByT5_ws3000             = 0.8873560947109903\n","# ByT5                    = 0.887649053358017\n","# ByT5_extra_layer        = 0.0\n","# MultiLexNorm_raw_data   = 0.7097791013001765\n","# MultiLexNorm_base       = 0.6818061444602583\n","# MultiLexNorm_Fine_tuned = 0.1129387766622926\n","\n","\n","## BLEU SCORES (normalized)\n","# Baseline (transcription)= 0.7871290040412282\n","# ByT5                    = 0.8233903148477896\n","# ByT5_lr_1e4             = 0.8320419867936647\n","# ByT5_ws3000             = 0.8231682536463248\n","# ByT5_extra_layer        = 0.0\n","# MultiLexNorm_raw_data   = 0.6473379750494206\n","# MultiLexNorm_base       = 0.6190157431613983\n","# MultiLexNorm_fine_tuned = 0.006133337391333845\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Evaluation.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"}},"nbformat":4,"nbformat_minor":0}
