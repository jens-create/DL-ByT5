{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies and load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torchmetrics==0.4.1\n",
    "!pip3 install transformers==4.8.2\n",
    "!pip3 install pytorch_lightning==1.3.8\n",
    "!pip3 install nltk\n",
    "!pip3 install Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from MLN_individual_files.helper_classes import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "from transformers import get_scheduler\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview data and fine-tune MLN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 706/706 [00:00<00:00, 227kB/s]\n",
      "Downloading: 100%|██████████| 1.20G/1.20G [01:47<00:00, 11.1MB/s] \n",
      "Downloading: 100%|██████████| 2.59k/2.59k [00:00<00:00, 1.07MB/s]\n",
      "Downloading: 100%|██████████| 2.50k/2.50k [00:00<00:00, 801kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('ufal/byt5-small-multilexnorm2021-da')\n",
    "tokenizer = AutoTokenizer.from_pretrained('ufal/byt5-small-multilexnorm2021-da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized...\n"
     ]
    }
   ],
   "source": [
    "with open('data/mln_data_test_inputs.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "with open('data/mln_data_test_outputs.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "data = MultiPlexDataset(X_train, y_train, only_include_corrections=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo the test set is loaded. In the real training we ofcourse used the training set. \n",
    "We have only included data points with errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_sample': 'gyset der har siddet sammenkrøbet i <extra_id_0>nakke<extra_id_1> regionen udløses',\n",
       " 'expected_output': 'nakkeregionen'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0] #We see that there is an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, batch_size=8, collate_fn=CollateFunctor_Train(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=0.3e-3)\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=4000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "running_loss=0\n",
    "model.train()\n",
    "for i, batch in enumerate(dataloader):\n",
    "    batch = {k: v.to(device) for k, v in batch.items() if k != 'sentence_ids' and k != 'word_ids'}\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    progress_bar.update(1)\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 50 == 0:    # print every 2000 mini-batches\n",
    "        print('[%5d] loss: %.3f' %\n",
    "              (i + 1, running_loss / 50))\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLN model was fine-tuned and the model was uploaded to huggingface.\n",
    "https://huggingface.co/jenspt/mln_ft "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do inferring on the fine-tuned model. We download it from the huggingface library. If we wanted to get a baseline using the MLN as it is, we could simply download the one that the MLN team has made available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('jenspt/mln_ft')\n",
    "tokenizer = AutoTokenizer.from_pretrained('ufal/byt5-small-multilexnorm2021-da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/mln_data_test_inputs.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "with open('data/mln_data_test_outputs.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "data = MultilexnormDataset(X_test, y_test)\n",
    "data_loader = get_train_dataloader(data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.apply_func import move_data_to_device\n",
    "output_dir = \"drive/My Drive/projekt/\"\n",
    "assembler = OutputAssembler(output_dir, data)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "for i, batch in enumerate(data_loader):\n",
    "    batch = move_data_to_device(batch, device)\n",
    "    sentence_ids, word_ids = batch[\"sentence_ids\"], batch[\"word_ids\"]\n",
    "    output = model.generate(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],\n",
    "            repetition_penalty=1.0, length_penalty=1.0, max_length=32,\n",
    "            num_beams=1, num_return_sequences=1,\n",
    "            output_scores=True, return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "    scores = [[0.0] for i in range(len(sentence_ids))]\n",
    "    outputs = tokenizer.batch_decode(output.sequences, skip_special_tokens=True)\n",
    "    outputs = [outputs[i:(i+1)] for i in range(len(sentence_ids))]\n",
    "\n",
    "    out_dict = {\n",
    "        \"predictions\": outputs,\n",
    "        \"scores\": scores,\n",
    "        \"sentence_ids\": sentence_ids,\n",
    "        \"word_ids\": word_ids,\n",
    "    }\n",
    "    assembler.step(out_dict)\n",
    "    print(f\"{i} / {(len(data) + 8 - 1) // 8}\", flush=True)\n",
    "assembler.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the test set has been corrected and the results are saved as a .txt file. We now open it and calculate the WER, BLEU and GLEU scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### READ FILES\n",
    "\n",
    "inputs, outputs = open_dataset('data/outputs_mln_ft.txt')\n",
    "corrected = [' '.join(sentence) for sentence in outputs]\n",
    "transcribed = [' '.join(sentence) for sentence in inputs]\n",
    "\n",
    "with open('data/mln_data_test_outputs.pkl', 'rb') as f:\n",
    "    reference = pickle.load(f)\n",
    "\n",
    "reference = [\" \".join(s) for s in reference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_text</th>\n",
       "      <th>transcription</th>\n",
       "      <th>corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gyset der har siddet sammenkrøbet i nakkeregio...</td>\n",
       "      <td>gyset der har siddet sammenkrøbet i nakke regi...</td>\n",
       "      <td>gylfi dér er sidde sammenkrøbene inden nakkere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>det er et enormt befolkningstal sammenlignet m...</td>\n",
       "      <td>det er et enormt befolkningstal sammenlignet m...</td>\n",
       "      <td>der er enormt enorm befolkningstal sammenligne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>de seks balletter er ikke alle avantgardestykk...</td>\n",
       "      <td>de seks balletter er ikke alle avangard stykke...</td>\n",
       "      <td>det seksballetter balletter har alle al avanga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stakkels davedarling</td>\n",
       "      <td>stakkels dave darling</td>\n",
       "      <td>stakels davedarling darling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>det får han osse</td>\n",
       "      <td>det får han også</td>\n",
       "      <td>dét for hr osse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      reference_text  \\\n",
       "0  gyset der har siddet sammenkrøbet i nakkeregio...   \n",
       "1  det er et enormt befolkningstal sammenlignet m...   \n",
       "2  de seks balletter er ikke alle avantgardestykk...   \n",
       "3                              stakkels davedarling    \n",
       "4                                   det får han osse   \n",
       "\n",
       "                                       transcription  \\\n",
       "0  gyset der har siddet sammenkrøbet i nakke regi...   \n",
       "1  det er et enormt befolkningstal sammenlignet m...   \n",
       "2  de seks balletter er ikke alle avangard stykke...   \n",
       "3                              stakkels dave darling   \n",
       "4                                   det får han også   \n",
       "\n",
       "                                           corrected  \n",
       "0  gylfi dér er sidde sammenkrøbene inden nakkere...  \n",
       "1  der er enormt enorm befolkningstal sammenligne...  \n",
       "2  det seksballetter balletter har alle al avanga...  \n",
       "3                        stakels davedarling darling  \n",
       "4                                    dét for hr osse  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mln_df = pd.DataFrame(list(zip(reference, transcribed, corrected)),\n",
    "               columns =['reference_text', 'transcription', 'corrected'])\n",
    "mln_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_text</th>\n",
       "      <th>transcription</th>\n",
       "      <th>corrected</th>\n",
       "      <th>corrected_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gyset der har siddet sammenkrøbet i nakkeregio...</td>\n",
       "      <td>gyset der har siddet sammenkrøbet i nakke regi...</td>\n",
       "      <td>gylfi dér er sidde sammenkrøbene inden nakkere...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>det er et enormt befolkningstal sammenlignet m...</td>\n",
       "      <td>det er et enormt befolkningstal sammenlignet m...</td>\n",
       "      <td>der er enormt enorm befolkningstal sammenligne...</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>de seks balletter er ikke alle avantgardestykk...</td>\n",
       "      <td>de seks balletter er ikke alle avangard stykke...</td>\n",
       "      <td>det seksballetter balletter har alle al avanga...</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stakkels davedarling</td>\n",
       "      <td>stakkels dave darling</td>\n",
       "      <td>stakels davedarling darling</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>det får han osse</td>\n",
       "      <td>det får han også</td>\n",
       "      <td>dét for hr osse</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      reference_text  \\\n",
       "0  gyset der har siddet sammenkrøbet i nakkeregio...   \n",
       "1  det er et enormt befolkningstal sammenlignet m...   \n",
       "2  de seks balletter er ikke alle avantgardestykk...   \n",
       "3                              stakkels davedarling    \n",
       "4                                   det får han osse   \n",
       "\n",
       "                                       transcription  \\\n",
       "0  gyset der har siddet sammenkrøbet i nakke regi...   \n",
       "1  det er et enormt befolkningstal sammenlignet m...   \n",
       "2  de seks balletter er ikke alle avangard stykke...   \n",
       "3                              stakkels dave darling   \n",
       "4                                   det får han også   \n",
       "\n",
       "                                           corrected  corrected_wer  \n",
       "0  gylfi dér er sidde sammenkrøbene inden nakkere...       0.666667  \n",
       "1  der er enormt enorm befolkningstal sammenligne...       0.642857  \n",
       "2  det seksballetter balletter har alle al avanga...       0.700000  \n",
       "3                        stakels davedarling darling       0.666667  \n",
       "4                                    dét for hr osse       0.750000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mln_df['corrected_wer'] = mln_df.apply(calculate_wer,axis=1)\n",
    "mln_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.16467803146487"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mln_df.corrected_wer.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/homebrew/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/homebrew/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006133337391333845\n",
      "0.1129387766622926\n"
     ]
    }
   ],
   "source": [
    "print(calculate_bleu_normalized(mln_df))\n",
    "print(calculate_gleu_normalized(mln_df))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
